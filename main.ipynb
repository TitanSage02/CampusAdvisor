{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Install requirements"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install -r requirements.txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'pysqlite3'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[11], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[38;5;66;03m#__import__('pysqlite3')\u001b[39;00m\n\u001b[0;32m      2\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01msys\u001b[39;00m\n\u001b[1;32m----> 3\u001b[0m sys\u001b[38;5;241m.\u001b[39mmodules[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msqlite3\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[43msys\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmodules\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpop\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mpysqlite3\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m \u001b[38;5;21;01mstreamlit\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m \u001b[38;5;21;01mst\u001b[39;00m\n\u001b[0;32m      6\u001b[0m \u001b[38;5;66;03m#Importations\u001b[39;00m\n",
      "\u001b[1;31mKeyError\u001b[0m: 'pysqlite3'"
     ]
    }
   ],
   "source": [
    "__import__('pysqlite3')\n",
    "import sys\n",
    "sys.modules['sqlite3'] = sys.modules.pop('pysqlite3')\n",
    "import streamlit as st\n",
    "\n",
    "#Importations\n",
    "from PIL import Image\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\", category=FutureWarning)\n",
    "\n",
    "import os, tempfile, glob, random\n",
    "from pathlib import Path\n",
    "from getpass import getpass\n",
    "import numpy as np\n",
    "from itertools import combinations\n",
    "from langchain.memory import ConversationSummaryBufferMemory,ConversationBufferMemory # type: ignore\n",
    "\n",
    "# LLM: Google_genai\n",
    "from langchain_google_genai import ChatGoogleGenerativeAI\n",
    "from langchain_google_genai import GoogleGenerativeAIEmbeddings\n",
    "\n",
    "# LLM: HuggingFace\n",
    "from langchain_community.embeddings import HuggingFaceInferenceAPIEmbeddings\n",
    "from langchain_community.llms import HuggingFaceHub\n",
    "\n",
    "# langchain prompts, memory, chains...\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.chains import ConversationalRetrievalChain\n",
    "\n",
    "from langchain_community.chat_message_histories import StreamlitChatMessageHistory\n",
    "\n",
    "from operator import itemgetter\n",
    "from langchain.memory import ConversationBufferMemory\n",
    "from langchain_core.runnables import RunnableLambda, RunnableParallel, RunnablePassthrough\n",
    "from langchain.schema import Document, format_document\n",
    "from langchain_core.messages import AIMessage, HumanMessage, get_buffer_string\n",
    "\n",
    "# Document loaders\n",
    "from langchain_community.document_loaders import (\n",
    "    TextLoader,\n",
    ")\n",
    "\n",
    "# Text Splitter\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, CharacterTextSplitter\n",
    "\n",
    "# OutputParser\n",
    "from langchain_core.output_parsers import StrOutputParser\n",
    "\n",
    "# Chroma: vectorstore\n",
    "from langchain_community.vectorstores import Chroma\n",
    "\n",
    "# Contextual Compression\n",
    "from langchain.retrievers.document_compressors import DocumentCompressorPipeline # type: ignore\n",
    "from langchain.text_splitter import CharacterTextSplitter\n",
    "from langchain_community.document_transformers import EmbeddingsRedundantFilter,LongContextReorder\n",
    "from langchain.retrievers.document_compressors import EmbeddingsFilter # type: ignore\n",
    "from langchain.retrievers import ContextualCompressionRetriever\n",
    "from langchain_community.document_loaders import TextLoader\n",
    "\n",
    "loader = TextLoader(\"data\")\n",
    "documents = loader.load()\n",
    "\n",
    "# Create a RecursiveCharacterTextSplitter\n",
    "text_splitter = RecursiveCharacterTextSplitter(\n",
    "    separators = [\"\\n\\n\", \"\\n\", \" \", \"\"],\n",
    "    chunk_size = 1600,\n",
    "    chunk_overlap= 200\n",
    ")\n",
    "\n",
    "# Text splitting\n",
    "chunks = text_splitter.split_documents(documents=documents)\n",
    "print(f\"Number of chunks: {len(chunks)}\")\n",
    "\n",
    "def select_embeddings_model(LLM_service=\"Google\"):\n",
    "    \"\"\"Connect to the embeddings API endpoint by specifying the name of the embedding model.\"\"\"\n",
    "    # if LLM_service == \"OpenAI\":\n",
    "    #     embeddings = OpenAIEmbeddings(\n",
    "    #         model='text-embedding-ada-002',\n",
    "    #         api_key=openai_api_key)\n",
    "\n",
    "    if LLM_service == \"Google\":\n",
    "        embeddings = GoogleGenerativeAIEmbeddings(\n",
    "            model=\"models/embedding-001\",\n",
    "            google_api_key=\"AIzaSyCMA5G8t2UnD5q92arsYrwAbSm19xZlZV4\",\n",
    "            timeout=300 # type: ignore\n",
    "        )\n",
    "        \n",
    "    # if LLM_service == \"HuggingFace\":\n",
    "    #     embeddings = HuggingFaceInferenceAPIEmbeddings(\n",
    "    #         api_key=\"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\",\n",
    "    #         model_name=\"sentence-transformers/all-MiniLM-L12-v2\"\n",
    "    #     )\n",
    "\n",
    "    return embeddings\n",
    "\n",
    "embeddings_google = select_embeddings_model(LLM_service=\"HuggingFace\")\n",
    "#embeddings_HuggingFace = select_embeddings_model(LLM_service=\"HuggingFace\")\n",
    "\n",
    "def create_vectorstore(embeddings, documents, vectorstore_name):\n",
    "    \"\"\"Create a Chroma vector database.\"\"\"\n",
    "    LOCAL_VECTOR_STORE_DIR = Path(\"vector_store\")\n",
    "    persist_directory = (LOCAL_VECTOR_STORE_DIR.as_posix() + \"/\" + vectorstore_name)\n",
    "    \n",
    "    vector_store = Chroma.from_documents(\n",
    "        documents=documents,\n",
    "        embedding=embeddings,\n",
    "        persist_directory=persist_directory\n",
    "    )\n",
    "    \n",
    "    return vector_store\n",
    "\n",
    "\n",
    "vector_store_google = create_vectorstore(\n",
    "        embeddings=embeddings_google,\n",
    "        documents = chunks,\n",
    "        vectorstore_name=\"Lead\"\n",
    "    )\n",
    "\n",
    "print(\"vector_store_google:\",vector_store_google._collection.count(),\" chunks.\")\n",
    "\n",
    "def Vectorstore_backed_retriever(vectorstore,search_type=\"similarity\", k=5, score_threshold=None):\n",
    "    \"\"\"create a vectorsore-backed retriever\n",
    "    Parameters:\n",
    "        search_type: Defines the type of search that the Retriever should perform.\n",
    "            Can be \"similarity\" (default), \"mmr\", or \"similarity_score_threshold\"\n",
    "        k: number of documents to return (Default: 5)\n",
    "        score_threshold: Minimum relevance threshold for similarity_score_threshold (default=None)\n",
    "    \"\"\"\n",
    "    search_kwargs={}\n",
    "\n",
    "    if k is not None:\n",
    "        search_kwargs['k'] = k\n",
    "    \n",
    "    if score_threshold is not None:\n",
    "        search_kwargs['score_threshold'] = score_threshold\n",
    "\n",
    "    retriever = vectorstore.as_retriever(\n",
    "        search_type=search_type,\n",
    "        search_kwargs=search_kwargs\n",
    "    )\n",
    "    return retriever\n",
    "\n",
    "\n",
    "base_retriever_google = Vectorstore_backed_retriever(vector_store_google, \"similarity\", k=5)\n",
    "\n",
    "def instantiate_LLM(LLM_provider=\"HuggingFace\",api_key=\"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\", temperature=0.5, top_p=0.95, model_name=None):\n",
    "    \"\"\"Instantiate LLM in Langchain.\n",
    "    Parameters:\n",
    "        LLM_provider (str): the LLM provider; in [\"OpenAI\",\"Google\",\"HuggingFace\"]\n",
    "        model_name (str): in [\"gpt-3.5-turbo\", \"gpt-3.5-turbo-0125\", \"gpt-4-turbo-preview\",\n",
    "            \"gemini-pro\", \"mistralai/Mistral-7B-Instruct-v0.2\"].\n",
    "        api_key (str): google_api_key or openai_api_key or huggingfacehub_api_token\n",
    "        temperature (float): Range: 0.0 - 1.0; default = 0.5\n",
    "        top_p (float): : Range: 0.0 - 1.0; default = 1.\n",
    "    \"\"\"\n",
    "    # if LLM_provider == \"OpenAI\":\n",
    "    #     llm = ChatOpenAI(\n",
    "    #         api_key=api_key,\n",
    "    #         model=model_name,\n",
    "    #         temperature=temperature,\n",
    "    #         model_kwargs={\n",
    "    #             \"top_p\": top_p\n",
    "    #         }\n",
    "    #     )\n",
    "    if LLM_provider == \"Google\":\n",
    "        llm = ChatGoogleGenerativeAI(\n",
    "            google_api_key=api_key,\n",
    "            # model=\"gemini-pro\",\n",
    "            model=model_name,\n",
    "            temperature=temperature,\n",
    "            top_p=top_p,\n",
    "            convert_system_message_to_human=True    \n",
    "        ) # type: ignore\n",
    "\n",
    "    if LLM_provider == \"HuggingFace\":\n",
    "        llm = HuggingFaceHub(\n",
    "            # repo_id=\"mistralai/Mistral-7B-Instruct-v0.2\",\n",
    "            repo_id=\"meta-llama/Meta-Llama-3-8B-Instruct\",\n",
    "            huggingfacehub_api_token=api_key,\n",
    "            model_kwargs={\n",
    "                \"temperature\":temperature,\n",
    "                \"top_p\": top_p,\n",
    "                \"do_sample\": True,\n",
    "                \"max_new_tokens\":1024\n",
    "            },\n",
    "        )\n",
    "    return llm\n",
    "\n",
    "def create_memory(model_name='gpt-3.5-turbo', memory_max_token=None):\n",
    "    \"\"\"Creates a ConversationSummaryBufferMemory for gpt-3.5-turbo\n",
    "    Creates a ConversationBufferMemory for the other models.\"\"\"\n",
    "\n",
    "    if model_name==\"gpt-3.5-turbo\":\n",
    "        if memory_max_token is None:\n",
    "        #     memory_max_token = 1024 # max_tokens for 'gpt-3.5-turbo' = 4096\n",
    "        # memory = ConversationSummaryBufferMemory(\n",
    "        #     max_token_limit=memory_max_token,\n",
    "        #     llm=ChatOpenAI(model_name=\"gpt-3.5-turbo\",openai_api_key=openai_api_key,temperature=0.1),\n",
    "        #     return_messages=True,\n",
    "        #     memory_key='chat_history',\n",
    "        #     output_key=\"answer\",\n",
    "        #     input_key=\"question\"\n",
    "        # )\n",
    "            pass\n",
    "    else:\n",
    "        memory = ConversationBufferMemory(\n",
    "            return_messages=True,\n",
    "            memory_key='chat_history',\n",
    "            output_key=\"answer\",\n",
    "            input_key=\"question\",\n",
    "        )\n",
    "    return memory\n",
    "\n",
    "\n",
    "def answer_template(language=\"french\"):\n",
    "    \"\"\"Pass the standalone question along with the chat history and context (retrieved documents) to the `LLM` to get an answer.\"\"\"\n",
    "\n",
    "    template = f\"\"\"\n",
    "    Vous êtes un guide scolaire pour l'Université d'Abomey-Calavi. Votre rôle est d'aider les jeunes bacheliers à choisir leurs études en fonction des programmes offerts par l'université. Vous devez utiliser les données sur les filières, les critères d'admission, les débouchés professionnels, et les opportunités disponibles. Assurez-vous de fournir des informations précises et pertinentes pour chaque programme mentionné. Les recommandations doivent toujours être spécifiques à l'Université d'Abomey-Calavi.\n",
    "\n",
    "    Answer the question at the end, using only the following context (delimited by <context></context>).\n",
    "    Your answer must be in the language at the end.\n",
    "\n",
    "    <context>\n",
    "    {{chat_history}}\n",
    "\n",
    "    {{context}}\n",
    "    </context>\n",
    "\n",
    "    Question: {{question}}\n",
    "\n",
    "    Language: {{language}}.\n",
    "    \n",
    "    \"\"\"\n",
    "\n",
    "    return template\n",
    "\n",
    "\n",
    "def _combine_documents(docs, document_prompt, document_separator=\"\\n\\n\"):\n",
    "    doc_strings = [format_document(doc, document_prompt) for doc in docs]\n",
    "    return document_separator.join(doc_strings)\n",
    "\n",
    "def custom_ConversationalRetrievalChain(\n",
    "    llm,condense_question_llm,\n",
    "    retriever,\n",
    "    language=\"french\",\n",
    "    llm_provider=\"OpenAI\",\n",
    "    model_name='gpt-3.5-turbo',\n",
    "):\n",
    "    \"\"\"Create a ConversationalRetrievalChain step by step.\n",
    "    \"\"\"\n",
    "    ##############################################################\n",
    "    # Step 1: Create a standalone_question chain\n",
    "    ##############################################################\n",
    "\n",
    "    # 1. Create memory: ConversationSummaryBufferMemory for gpt-3.5, and ConversationBufferMemory for the other models\n",
    "\n",
    "    memory = create_memory(model_name)\n",
    "    # memory = ConversationBufferMemory(memory_key=\"chat_history\",output_key=\"answer\", input_key=\"question\",return_messages=True)\n",
    "\n",
    "    # 2. load memory using RunnableLambda. Retrieves the chat_history attribute using itemgetter.\n",
    "    loaded_memory = RunnablePassthrough.assign(\n",
    "        chat_history=RunnableLambda(memory.load_memory_variables) | itemgetter(\"chat_history\"),\n",
    "    )\n",
    "\n",
    "    # 3. Pass the follow-up question along with the chat history to the LLM, and parse the answer (standalone_question).\n",
    "\n",
    "    condense_question_prompt = PromptTemplate(\n",
    "        input_variables=['chat_history', 'question'],\n",
    "        template = \"\"\"Given the following conversation and a follow up question,\n",
    "rephrase the follow up question to be a standalone question, in the same language as the follow up question.\\n\\n\n",
    "Chat History:\\n{chat_history}\\n\n",
    "Follow Up Input: {question}\\n\n",
    "Standalone question:\"\"\"\n",
    ")\n",
    "\n",
    "    standalone_question_chain = {\n",
    "        \"standalone_question\": {\n",
    "            \"question\": lambda x: x[\"question\"],\n",
    "            \"chat_history\": lambda x: get_buffer_string(x[\"chat_history\"]),\n",
    "        }\n",
    "        | condense_question_prompt\n",
    "        | condense_question_llm\n",
    "        | StrOutputParser(),\n",
    "    }\n",
    "\n",
    "    # 4. Combine load_memory and standalone_question_chain\n",
    "    chain_question = loaded_memory | standalone_question_chain\n",
    "\n",
    "    ####################################################################################\n",
    "    #   Step 2: Retrieve documents, pass them to the LLM, and return the response.\n",
    "    ####################################################################################\n",
    "\n",
    "    # 5. Retrieve relevant documents\n",
    "    retrieved_documents = {\n",
    "        \"docs\": itemgetter(\"standalone_question\") | retriever,\n",
    "        \"question\": lambda x: x[\"standalone_question\"],\n",
    "    }\n",
    "\n",
    "    # 6. Get variables ['chat_history', 'context', 'question'] that will be passed to `answer_prompt`\n",
    "\n",
    "    DEFAULT_DOCUMENT_PROMPT = PromptTemplate.from_template(template=\"{page_content}\")\n",
    "    answer_prompt = ChatPromptTemplate.from_template(answer_template(language=language))\n",
    "    # 3 variables are expected ['chat_history', 'context', 'question'] by the ChatPromptTemplate\n",
    "    answer_prompt_variables = {\n",
    "        \"context\": lambda x: _combine_documents(docs=x[\"docs\"],document_prompt=DEFAULT_DOCUMENT_PROMPT),\n",
    "        \"question\": itemgetter(\"question\"),\n",
    "        \"chat_history\": itemgetter(\"chat_history\") # get it from `loaded_memory` variable\n",
    "    }\n",
    "\n",
    "    # 7. Load memory, format `answer_prompt` with variables (context, question and chat_history) and pass the `answer_prompt to LLM.\n",
    "    # return answer, docs and standalone_question\n",
    "\n",
    "    chain_answer = {\n",
    "        \"answer\": loaded_memory | answer_prompt_variables | answer_prompt | llm,\n",
    "        # return only page_content and metadata\n",
    "        \"docs\": lambda x: [Document(page_content=doc.page_content,metadata=doc.metadata) for doc in x[\"docs\"]],\n",
    "        \"standalone_question\": lambda x:x[\"question\"] # return standalone_question\n",
    "    }\n",
    "\n",
    "    # 8. Final chain\n",
    "    conversational_retriever_chain = chain_question | retrieved_documents | chain_answer\n",
    "\n",
    "    print(\"Conversational retriever chain created successfully!\")\n",
    "\n",
    "    return conversational_retriever_chain,memory\n",
    "\n",
    "\n",
    "chain_gemini, memory_gemini = custom_ConversationalRetrievalChain(\n",
    "    llm = instantiate_LLM(\n",
    "            LLM_provider=\"Google\", api_key=\"AIzaSyCMA5G8t2UnD5q92arsYrwAbSm19xZlZV4\", temperature=0.5,model_name=\"gemini-1.5-pro\"),\n",
    "    \n",
    "    condense_question_llm = instantiate_LLM(\n",
    "            LLM_provider=\"Google\", api_key=\"AIzaSyCMA5G8t2UnD5q92arsYrwAbSm19xZlZV4\", temperature=0.1,model_name=\"gemini-1.5-pro\"),\n",
    "    \n",
    "    retriever=base_retriever_google,\n",
    "    language=\"french\",\n",
    "    llm_provider=\"Google\",\n",
    "    model_name=\"gemini-1.5-pro\"\n",
    ")\n",
    "\n",
    "\n",
    "with st.sidebar:\n",
    "    reset_button_key = \"reset_button\"\n",
    "    reset_button = st.button(\"Reset Chat\",key=reset_button_key)\n",
    "    if reset_button:\n",
    "        st.session_state.conversation = None\n",
    "        st.session_state.chat_history = None\n",
    "    \"[View the source code](https://github.com/Espe-dev)\"\n",
    "    \"[![Open in GitHub Codespaces](https://github.com/codespaces/badge.svg)](https://github.com/Espe-dev)\"\n",
    "st.image('logo.jpeg', width=200)\n",
    "\n",
    "st.caption(\"🚀 Bienvenue CampusAdvisor, ton guide universitaire !\")\n",
    "\n",
    "if \"messages\" not in st.session_state:\n",
    "    st.session_state[\"messages\"] = [{\"role\": \"assistant\", \"content\": \"En quoi pouvons nous vous aider ?\"}]\n",
    "\n",
    "for msg in st.session_state.messages:\n",
    "    st.chat_message(msg[\"role\"]).write(msg[\"content\"])\n",
    "\n",
    "if prompt := st.chat_input():\n",
    "    st.session_state.messages.append({\"role\": \"user\", \"content\": prompt})\n",
    "    st.chat_message(\"user\").write(prompt)\n",
    "    \n",
    "    # Afficher un spinner pendant le traitement de la question\n",
    "    with st.spinner('Traitement de la question en cours...'):\n",
    "        response = chain_gemini.invoke({\"question\":prompt})\n",
    "        \n",
    "    msg = response['answer'].content\n",
    "    st.session_state.messages.append({\"role\": \"assistant\", \"content\": msg})\n",
    "    \n",
    "    # Afficher un spinner pendant la génération de la réponse    \n",
    "    with st.spinner('Génération de la réponse en cours...'):\n",
    "        st.chat_message(\"assistant\").write(msg)\n",
    "        \n",
    "    memory_gemini.save_context({\"question\": prompt}, {\"answer\": msg})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
